{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e09cbff",
   "metadata": {},
   "source": [
    "# MAM02 SVMs and Random Forest\n",
    "Students: Dilara Tank, Martijn Siepel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c94d59f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3153d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "raw_train = pd.read_csv(\"simulatedConcentrations.recoded.training.txt\", sep = \"\\t\")\n",
    "raw_test = pd.read_csv(\"simulatedConcentrations.recoded.test.txt\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a878c33b",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "There are 700 rows and 251 columns in the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b877a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cc</th>\n",
       "      <th>p_1</th>\n",
       "      <th>p_2</th>\n",
       "      <th>p_3</th>\n",
       "      <th>p_4</th>\n",
       "      <th>p_5</th>\n",
       "      <th>p_6</th>\n",
       "      <th>p_7</th>\n",
       "      <th>p_8</th>\n",
       "      <th>p_9</th>\n",
       "      <th>...</th>\n",
       "      <th>p_241</th>\n",
       "      <th>p_242</th>\n",
       "      <th>p_243</th>\n",
       "      <th>p_244</th>\n",
       "      <th>p_245</th>\n",
       "      <th>p_246</th>\n",
       "      <th>p_247</th>\n",
       "      <th>p_248</th>\n",
       "      <th>p_249</th>\n",
       "      <th>p_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5.990080</td>\n",
       "      <td>2.372956</td>\n",
       "      <td>1.863723</td>\n",
       "      <td>1.912556</td>\n",
       "      <td>2.447490</td>\n",
       "      <td>3.805296</td>\n",
       "      <td>0.079518</td>\n",
       "      <td>2.735940</td>\n",
       "      <td>2.242411</td>\n",
       "      <td>...</td>\n",
       "      <td>4.178314</td>\n",
       "      <td>2.842667</td>\n",
       "      <td>1.558608</td>\n",
       "      <td>4.244815</td>\n",
       "      <td>3.994065</td>\n",
       "      <td>3.617629</td>\n",
       "      <td>3.232601</td>\n",
       "      <td>1.196992</td>\n",
       "      <td>3.599879</td>\n",
       "      <td>1.273826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3.876533</td>\n",
       "      <td>3.761698</td>\n",
       "      <td>2.072886</td>\n",
       "      <td>2.690199</td>\n",
       "      <td>0.682463</td>\n",
       "      <td>2.717090</td>\n",
       "      <td>0.413700</td>\n",
       "      <td>4.269970</td>\n",
       "      <td>2.273345</td>\n",
       "      <td>...</td>\n",
       "      <td>3.310263</td>\n",
       "      <td>1.444818</td>\n",
       "      <td>4.577748</td>\n",
       "      <td>4.148891</td>\n",
       "      <td>3.870811</td>\n",
       "      <td>4.334630</td>\n",
       "      <td>3.485450</td>\n",
       "      <td>2.558260</td>\n",
       "      <td>2.864337</td>\n",
       "      <td>2.070249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3.417281</td>\n",
       "      <td>3.458750</td>\n",
       "      <td>3.394488</td>\n",
       "      <td>4.422391</td>\n",
       "      <td>2.548079</td>\n",
       "      <td>2.944549</td>\n",
       "      <td>3.387399</td>\n",
       "      <td>4.615141</td>\n",
       "      <td>3.167030</td>\n",
       "      <td>...</td>\n",
       "      <td>3.804504</td>\n",
       "      <td>3.866382</td>\n",
       "      <td>2.940198</td>\n",
       "      <td>4.159919</td>\n",
       "      <td>3.694627</td>\n",
       "      <td>2.976894</td>\n",
       "      <td>3.460826</td>\n",
       "      <td>4.145632</td>\n",
       "      <td>1.790922</td>\n",
       "      <td>2.547390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4.651434</td>\n",
       "      <td>2.695008</td>\n",
       "      <td>1.493320</td>\n",
       "      <td>4.594457</td>\n",
       "      <td>1.903162</td>\n",
       "      <td>0.913204</td>\n",
       "      <td>3.281215</td>\n",
       "      <td>3.124500</td>\n",
       "      <td>4.862742</td>\n",
       "      <td>...</td>\n",
       "      <td>1.843456</td>\n",
       "      <td>2.350108</td>\n",
       "      <td>4.497963</td>\n",
       "      <td>3.332925</td>\n",
       "      <td>2.157612</td>\n",
       "      <td>3.951805</td>\n",
       "      <td>3.224491</td>\n",
       "      <td>1.376894</td>\n",
       "      <td>2.733858</td>\n",
       "      <td>1.433679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2.186504</td>\n",
       "      <td>3.473139</td>\n",
       "      <td>1.830540</td>\n",
       "      <td>3.105082</td>\n",
       "      <td>2.572080</td>\n",
       "      <td>4.591407</td>\n",
       "      <td>1.209103</td>\n",
       "      <td>3.311880</td>\n",
       "      <td>2.824717</td>\n",
       "      <td>...</td>\n",
       "      <td>2.022999</td>\n",
       "      <td>2.570952</td>\n",
       "      <td>4.405190</td>\n",
       "      <td>4.225326</td>\n",
       "      <td>2.686949</td>\n",
       "      <td>2.259473</td>\n",
       "      <td>3.928619</td>\n",
       "      <td>4.852162</td>\n",
       "      <td>4.456258</td>\n",
       "      <td>3.135190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>1</td>\n",
       "      <td>2.694740</td>\n",
       "      <td>5.172024</td>\n",
       "      <td>2.365657</td>\n",
       "      <td>3.675559</td>\n",
       "      <td>2.985784</td>\n",
       "      <td>4.584865</td>\n",
       "      <td>2.361670</td>\n",
       "      <td>1.994775</td>\n",
       "      <td>2.136689</td>\n",
       "      <td>...</td>\n",
       "      <td>3.812098</td>\n",
       "      <td>3.046719</td>\n",
       "      <td>2.939828</td>\n",
       "      <td>2.328593</td>\n",
       "      <td>3.759822</td>\n",
       "      <td>1.683018</td>\n",
       "      <td>2.975785</td>\n",
       "      <td>4.235410</td>\n",
       "      <td>0.797459</td>\n",
       "      <td>3.380627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>1</td>\n",
       "      <td>2.151744</td>\n",
       "      <td>4.682264</td>\n",
       "      <td>2.566471</td>\n",
       "      <td>4.302707</td>\n",
       "      <td>2.854239</td>\n",
       "      <td>2.787095</td>\n",
       "      <td>1.726916</td>\n",
       "      <td>1.829500</td>\n",
       "      <td>3.047581</td>\n",
       "      <td>...</td>\n",
       "      <td>2.404283</td>\n",
       "      <td>4.183376</td>\n",
       "      <td>3.166567</td>\n",
       "      <td>2.621022</td>\n",
       "      <td>3.856908</td>\n",
       "      <td>1.958613</td>\n",
       "      <td>2.610078</td>\n",
       "      <td>3.453517</td>\n",
       "      <td>2.782237</td>\n",
       "      <td>3.704500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>1</td>\n",
       "      <td>3.581671</td>\n",
       "      <td>4.068131</td>\n",
       "      <td>3.499364</td>\n",
       "      <td>2.779425</td>\n",
       "      <td>4.065883</td>\n",
       "      <td>2.288258</td>\n",
       "      <td>3.796080</td>\n",
       "      <td>5.012222</td>\n",
       "      <td>4.168613</td>\n",
       "      <td>...</td>\n",
       "      <td>4.863811</td>\n",
       "      <td>2.409710</td>\n",
       "      <td>3.463669</td>\n",
       "      <td>3.144249</td>\n",
       "      <td>3.041745</td>\n",
       "      <td>4.361741</td>\n",
       "      <td>3.025681</td>\n",
       "      <td>1.116597</td>\n",
       "      <td>4.042750</td>\n",
       "      <td>2.844593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>1</td>\n",
       "      <td>4.663016</td>\n",
       "      <td>4.004986</td>\n",
       "      <td>3.046081</td>\n",
       "      <td>2.626760</td>\n",
       "      <td>3.375057</td>\n",
       "      <td>3.829221</td>\n",
       "      <td>3.370321</td>\n",
       "      <td>4.138768</td>\n",
       "      <td>4.971202</td>\n",
       "      <td>...</td>\n",
       "      <td>3.592864</td>\n",
       "      <td>3.151530</td>\n",
       "      <td>1.812748</td>\n",
       "      <td>1.735959</td>\n",
       "      <td>3.579429</td>\n",
       "      <td>1.813875</td>\n",
       "      <td>4.127705</td>\n",
       "      <td>3.095179</td>\n",
       "      <td>1.009377</td>\n",
       "      <td>5.152303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>1</td>\n",
       "      <td>4.014250</td>\n",
       "      <td>5.529675</td>\n",
       "      <td>2.985523</td>\n",
       "      <td>1.813008</td>\n",
       "      <td>0.973436</td>\n",
       "      <td>3.501147</td>\n",
       "      <td>3.073458</td>\n",
       "      <td>3.901621</td>\n",
       "      <td>3.015373</td>\n",
       "      <td>...</td>\n",
       "      <td>3.859663</td>\n",
       "      <td>4.449756</td>\n",
       "      <td>2.960318</td>\n",
       "      <td>3.369533</td>\n",
       "      <td>3.455361</td>\n",
       "      <td>2.387916</td>\n",
       "      <td>1.551928</td>\n",
       "      <td>1.918011</td>\n",
       "      <td>1.977655</td>\n",
       "      <td>3.315620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows × 251 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cc       p_1       p_2       p_3       p_4       p_5       p_6       p_7  \\\n",
       "0     0  5.990080  2.372956  1.863723  1.912556  2.447490  3.805296  0.079518   \n",
       "1     0  3.876533  3.761698  2.072886  2.690199  0.682463  2.717090  0.413700   \n",
       "2     0  3.417281  3.458750  3.394488  4.422391  2.548079  2.944549  3.387399   \n",
       "3     0  4.651434  2.695008  1.493320  4.594457  1.903162  0.913204  3.281215   \n",
       "4     0  2.186504  3.473139  1.830540  3.105082  2.572080  4.591407  1.209103   \n",
       "..   ..       ...       ...       ...       ...       ...       ...       ...   \n",
       "695   1  2.694740  5.172024  2.365657  3.675559  2.985784  4.584865  2.361670   \n",
       "696   1  2.151744  4.682264  2.566471  4.302707  2.854239  2.787095  1.726916   \n",
       "697   1  3.581671  4.068131  3.499364  2.779425  4.065883  2.288258  3.796080   \n",
       "698   1  4.663016  4.004986  3.046081  2.626760  3.375057  3.829221  3.370321   \n",
       "699   1  4.014250  5.529675  2.985523  1.813008  0.973436  3.501147  3.073458   \n",
       "\n",
       "          p_8       p_9  ...     p_241     p_242     p_243     p_244  \\\n",
       "0    2.735940  2.242411  ...  4.178314  2.842667  1.558608  4.244815   \n",
       "1    4.269970  2.273345  ...  3.310263  1.444818  4.577748  4.148891   \n",
       "2    4.615141  3.167030  ...  3.804504  3.866382  2.940198  4.159919   \n",
       "3    3.124500  4.862742  ...  1.843456  2.350108  4.497963  3.332925   \n",
       "4    3.311880  2.824717  ...  2.022999  2.570952  4.405190  4.225326   \n",
       "..        ...       ...  ...       ...       ...       ...       ...   \n",
       "695  1.994775  2.136689  ...  3.812098  3.046719  2.939828  2.328593   \n",
       "696  1.829500  3.047581  ...  2.404283  4.183376  3.166567  2.621022   \n",
       "697  5.012222  4.168613  ...  4.863811  2.409710  3.463669  3.144249   \n",
       "698  4.138768  4.971202  ...  3.592864  3.151530  1.812748  1.735959   \n",
       "699  3.901621  3.015373  ...  3.859663  4.449756  2.960318  3.369533   \n",
       "\n",
       "        p_245     p_246     p_247     p_248     p_249     p_250  \n",
       "0    3.994065  3.617629  3.232601  1.196992  3.599879  1.273826  \n",
       "1    3.870811  4.334630  3.485450  2.558260  2.864337  2.070249  \n",
       "2    3.694627  2.976894  3.460826  4.145632  1.790922  2.547390  \n",
       "3    2.157612  3.951805  3.224491  1.376894  2.733858  1.433679  \n",
       "4    2.686949  2.259473  3.928619  4.852162  4.456258  3.135190  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "695  3.759822  1.683018  2.975785  4.235410  0.797459  3.380627  \n",
       "696  3.856908  1.958613  2.610078  3.453517  2.782237  3.704500  \n",
       "697  3.041745  4.361741  3.025681  1.116597  4.042750  2.844593  \n",
       "698  3.579429  1.813875  4.127705  3.095179  1.009377  5.152303  \n",
       "699  3.455361  2.387916  1.551928  1.918011  1.977655  3.315620  \n",
       "\n",
       "[700 rows x 251 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c93e8f",
   "metadata": {},
   "source": [
    "There are 300 rows and 251 columns in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3fcfb81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cc</th>\n",
       "      <th>p_1</th>\n",
       "      <th>p_2</th>\n",
       "      <th>p_3</th>\n",
       "      <th>p_4</th>\n",
       "      <th>p_5</th>\n",
       "      <th>p_6</th>\n",
       "      <th>p_7</th>\n",
       "      <th>p_8</th>\n",
       "      <th>p_9</th>\n",
       "      <th>...</th>\n",
       "      <th>p_241</th>\n",
       "      <th>p_242</th>\n",
       "      <th>p_243</th>\n",
       "      <th>p_244</th>\n",
       "      <th>p_245</th>\n",
       "      <th>p_246</th>\n",
       "      <th>p_247</th>\n",
       "      <th>p_248</th>\n",
       "      <th>p_249</th>\n",
       "      <th>p_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.612468</td>\n",
       "      <td>3.474305</td>\n",
       "      <td>3.081643</td>\n",
       "      <td>2.647540</td>\n",
       "      <td>1.477145</td>\n",
       "      <td>2.226074</td>\n",
       "      <td>0.907987</td>\n",
       "      <td>1.930012</td>\n",
       "      <td>2.031522</td>\n",
       "      <td>...</td>\n",
       "      <td>3.012094</td>\n",
       "      <td>5.157420</td>\n",
       "      <td>3.271333</td>\n",
       "      <td>3.451251</td>\n",
       "      <td>3.058974</td>\n",
       "      <td>1.503359</td>\n",
       "      <td>1.734572</td>\n",
       "      <td>2.834744</td>\n",
       "      <td>3.686796</td>\n",
       "      <td>2.067522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.300964</td>\n",
       "      <td>3.075240</td>\n",
       "      <td>2.436726</td>\n",
       "      <td>4.035113</td>\n",
       "      <td>1.827510</td>\n",
       "      <td>3.886138</td>\n",
       "      <td>1.795661</td>\n",
       "      <td>1.664807</td>\n",
       "      <td>2.822648</td>\n",
       "      <td>...</td>\n",
       "      <td>4.514789</td>\n",
       "      <td>3.051476</td>\n",
       "      <td>3.925865</td>\n",
       "      <td>3.011847</td>\n",
       "      <td>3.392423</td>\n",
       "      <td>3.479198</td>\n",
       "      <td>2.213151</td>\n",
       "      <td>4.181237</td>\n",
       "      <td>3.711856</td>\n",
       "      <td>2.431092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.730748</td>\n",
       "      <td>3.798289</td>\n",
       "      <td>2.741223</td>\n",
       "      <td>3.831064</td>\n",
       "      <td>1.779807</td>\n",
       "      <td>1.371133</td>\n",
       "      <td>1.286970</td>\n",
       "      <td>4.026218</td>\n",
       "      <td>2.966135</td>\n",
       "      <td>...</td>\n",
       "      <td>3.890185</td>\n",
       "      <td>3.589208</td>\n",
       "      <td>3.702127</td>\n",
       "      <td>4.566962</td>\n",
       "      <td>4.200340</td>\n",
       "      <td>4.353270</td>\n",
       "      <td>3.298243</td>\n",
       "      <td>4.260439</td>\n",
       "      <td>2.615887</td>\n",
       "      <td>3.268334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.760209</td>\n",
       "      <td>2.877058</td>\n",
       "      <td>4.115963</td>\n",
       "      <td>3.297249</td>\n",
       "      <td>2.233364</td>\n",
       "      <td>2.165131</td>\n",
       "      <td>2.784745</td>\n",
       "      <td>3.226482</td>\n",
       "      <td>2.693925</td>\n",
       "      <td>...</td>\n",
       "      <td>3.943689</td>\n",
       "      <td>2.775926</td>\n",
       "      <td>4.189492</td>\n",
       "      <td>4.020823</td>\n",
       "      <td>2.355680</td>\n",
       "      <td>2.643243</td>\n",
       "      <td>3.660590</td>\n",
       "      <td>4.114268</td>\n",
       "      <td>2.521084</td>\n",
       "      <td>1.291745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2.335384</td>\n",
       "      <td>1.078580</td>\n",
       "      <td>2.289023</td>\n",
       "      <td>2.876278</td>\n",
       "      <td>2.435920</td>\n",
       "      <td>4.839064</td>\n",
       "      <td>4.033762</td>\n",
       "      <td>2.575286</td>\n",
       "      <td>2.611814</td>\n",
       "      <td>...</td>\n",
       "      <td>2.427503</td>\n",
       "      <td>3.331249</td>\n",
       "      <td>4.052921</td>\n",
       "      <td>1.686589</td>\n",
       "      <td>1.937887</td>\n",
       "      <td>2.529955</td>\n",
       "      <td>1.483750</td>\n",
       "      <td>2.489929</td>\n",
       "      <td>2.104198</td>\n",
       "      <td>3.536856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>1</td>\n",
       "      <td>3.018572</td>\n",
       "      <td>1.933102</td>\n",
       "      <td>3.382359</td>\n",
       "      <td>2.397873</td>\n",
       "      <td>3.112016</td>\n",
       "      <td>2.308696</td>\n",
       "      <td>2.674707</td>\n",
       "      <td>0.888944</td>\n",
       "      <td>2.409014</td>\n",
       "      <td>...</td>\n",
       "      <td>2.732044</td>\n",
       "      <td>3.351871</td>\n",
       "      <td>2.515280</td>\n",
       "      <td>3.299979</td>\n",
       "      <td>3.849993</td>\n",
       "      <td>4.483393</td>\n",
       "      <td>3.237941</td>\n",
       "      <td>2.427215</td>\n",
       "      <td>2.807415</td>\n",
       "      <td>2.906133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>1</td>\n",
       "      <td>3.308753</td>\n",
       "      <td>5.260347</td>\n",
       "      <td>2.917089</td>\n",
       "      <td>4.919993</td>\n",
       "      <td>1.979389</td>\n",
       "      <td>3.099847</td>\n",
       "      <td>2.010594</td>\n",
       "      <td>3.591778</td>\n",
       "      <td>0.327217</td>\n",
       "      <td>...</td>\n",
       "      <td>1.809449</td>\n",
       "      <td>2.298975</td>\n",
       "      <td>3.361137</td>\n",
       "      <td>2.190559</td>\n",
       "      <td>3.992505</td>\n",
       "      <td>3.045573</td>\n",
       "      <td>4.251126</td>\n",
       "      <td>3.943378</td>\n",
       "      <td>1.998725</td>\n",
       "      <td>2.070585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>1</td>\n",
       "      <td>2.930498</td>\n",
       "      <td>2.553745</td>\n",
       "      <td>3.279947</td>\n",
       "      <td>1.146571</td>\n",
       "      <td>2.969376</td>\n",
       "      <td>2.493065</td>\n",
       "      <td>4.488515</td>\n",
       "      <td>3.366241</td>\n",
       "      <td>1.200322</td>\n",
       "      <td>...</td>\n",
       "      <td>3.229196</td>\n",
       "      <td>4.270557</td>\n",
       "      <td>0.570573</td>\n",
       "      <td>1.942790</td>\n",
       "      <td>2.940316</td>\n",
       "      <td>4.197061</td>\n",
       "      <td>3.716778</td>\n",
       "      <td>2.315817</td>\n",
       "      <td>3.263704</td>\n",
       "      <td>0.918061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>1</td>\n",
       "      <td>1.269487</td>\n",
       "      <td>2.615980</td>\n",
       "      <td>1.549721</td>\n",
       "      <td>5.626274</td>\n",
       "      <td>2.785671</td>\n",
       "      <td>3.360088</td>\n",
       "      <td>2.980640</td>\n",
       "      <td>3.692053</td>\n",
       "      <td>3.110285</td>\n",
       "      <td>...</td>\n",
       "      <td>3.960706</td>\n",
       "      <td>4.160297</td>\n",
       "      <td>4.622836</td>\n",
       "      <td>2.379770</td>\n",
       "      <td>2.621007</td>\n",
       "      <td>2.680317</td>\n",
       "      <td>2.795090</td>\n",
       "      <td>3.815277</td>\n",
       "      <td>1.583589</td>\n",
       "      <td>4.019925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>1</td>\n",
       "      <td>2.168271</td>\n",
       "      <td>3.347003</td>\n",
       "      <td>3.033209</td>\n",
       "      <td>1.306114</td>\n",
       "      <td>2.410624</td>\n",
       "      <td>1.930745</td>\n",
       "      <td>3.253705</td>\n",
       "      <td>2.548122</td>\n",
       "      <td>2.426615</td>\n",
       "      <td>...</td>\n",
       "      <td>1.894271</td>\n",
       "      <td>4.347398</td>\n",
       "      <td>2.870187</td>\n",
       "      <td>2.611292</td>\n",
       "      <td>2.882806</td>\n",
       "      <td>4.311183</td>\n",
       "      <td>2.799349</td>\n",
       "      <td>1.442491</td>\n",
       "      <td>2.174876</td>\n",
       "      <td>3.863968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 251 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cc       p_1       p_2       p_3       p_4       p_5       p_6       p_7  \\\n",
       "0     0  1.612468  3.474305  3.081643  2.647540  1.477145  2.226074  0.907987   \n",
       "1     0  1.300964  3.075240  2.436726  4.035113  1.827510  3.886138  1.795661   \n",
       "2     0  0.730748  3.798289  2.741223  3.831064  1.779807  1.371133  1.286970   \n",
       "3     0  1.760209  2.877058  4.115963  3.297249  2.233364  2.165131  2.784745   \n",
       "4     0  2.335384  1.078580  2.289023  2.876278  2.435920  4.839064  4.033762   \n",
       "..   ..       ...       ...       ...       ...       ...       ...       ...   \n",
       "295   1  3.018572  1.933102  3.382359  2.397873  3.112016  2.308696  2.674707   \n",
       "296   1  3.308753  5.260347  2.917089  4.919993  1.979389  3.099847  2.010594   \n",
       "297   1  2.930498  2.553745  3.279947  1.146571  2.969376  2.493065  4.488515   \n",
       "298   1  1.269487  2.615980  1.549721  5.626274  2.785671  3.360088  2.980640   \n",
       "299   1  2.168271  3.347003  3.033209  1.306114  2.410624  1.930745  3.253705   \n",
       "\n",
       "          p_8       p_9  ...     p_241     p_242     p_243     p_244  \\\n",
       "0    1.930012  2.031522  ...  3.012094  5.157420  3.271333  3.451251   \n",
       "1    1.664807  2.822648  ...  4.514789  3.051476  3.925865  3.011847   \n",
       "2    4.026218  2.966135  ...  3.890185  3.589208  3.702127  4.566962   \n",
       "3    3.226482  2.693925  ...  3.943689  2.775926  4.189492  4.020823   \n",
       "4    2.575286  2.611814  ...  2.427503  3.331249  4.052921  1.686589   \n",
       "..        ...       ...  ...       ...       ...       ...       ...   \n",
       "295  0.888944  2.409014  ...  2.732044  3.351871  2.515280  3.299979   \n",
       "296  3.591778  0.327217  ...  1.809449  2.298975  3.361137  2.190559   \n",
       "297  3.366241  1.200322  ...  3.229196  4.270557  0.570573  1.942790   \n",
       "298  3.692053  3.110285  ...  3.960706  4.160297  4.622836  2.379770   \n",
       "299  2.548122  2.426615  ...  1.894271  4.347398  2.870187  2.611292   \n",
       "\n",
       "        p_245     p_246     p_247     p_248     p_249     p_250  \n",
       "0    3.058974  1.503359  1.734572  2.834744  3.686796  2.067522  \n",
       "1    3.392423  3.479198  2.213151  4.181237  3.711856  2.431092  \n",
       "2    4.200340  4.353270  3.298243  4.260439  2.615887  3.268334  \n",
       "3    2.355680  2.643243  3.660590  4.114268  2.521084  1.291745  \n",
       "4    1.937887  2.529955  1.483750  2.489929  2.104198  3.536856  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "295  3.849993  4.483393  3.237941  2.427215  2.807415  2.906133  \n",
       "296  3.992505  3.045573  4.251126  3.943378  1.998725  2.070585  \n",
       "297  2.940316  4.197061  3.716778  2.315817  3.263704  0.918061  \n",
       "298  2.621007  2.680317  2.795090  3.815277  1.583589  4.019925  \n",
       "299  2.882806  4.311183  2.799349  1.442491  2.174876  3.863968  \n",
       "\n",
       "[300 rows x 251 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6679c0de",
   "metadata": {},
   "source": [
    "The classes are equally distributed among the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36cff78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc = 0, train: 350\n",
      "cc = 1, train: 350\n",
      "cc = 0, test : 150\n",
      "cc = 1, test : 150\n"
     ]
    }
   ],
   "source": [
    "# sum all rows where cc = 0 in the training set\n",
    "print('cc = 0, train:', (raw_train['cc'] == 0).sum())\n",
    "# sum all rows where cc = 1 in the training set\n",
    "print('cc = 1, train:', (raw_train['cc'] == 1).sum())\n",
    "\n",
    "# sum all rows where cc = 0 in the test set\n",
    "print('cc = 0, test :', (raw_test['cc'] == 0).sum())\n",
    "# sum all rows where cc = 1 in the test set\n",
    "print('cc = 1, test :', (raw_test['cc'] == 1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15df3e83",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb93d5f",
   "metadata": {},
   "source": [
    "We fit sklearn's SVM to the data and allow probabilities. We show the amount of support vectors the model created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ff53015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(kernel = 'linear', probability=True)\n",
    "clf.fit(raw_train[raw_train.columns[1:]], raw_train[raw_train.columns[0]])\n",
    "len(clf.support_vectors_) # 216 in R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a2e6b6",
   "metadata": {},
   "source": [
    "We make predictions with probabilities and compute the AUC of the model. The ROC curve plots the sensitivity (true positive rate) against the specificity (true negative rate) of the data. The area under this curve (Area Under Curve -> AUC) says something about the ability of the classifier to distinguish between the positive (cc=1) and negative (cc=0) class. An AUC of 1 would indicate that the model correctly distinguishes between all positive and negative classes. An AUC between 0.5 and 1 means there is a high chance that the model will be able to distinguish between the classes. An AUC of 0.89 is therefore quite good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47c56f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8962222222222223"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = pd.DataFrame(clf.predict_proba(raw_test[raw_test.columns[1:]]))[1] # with probabilities\n",
    "# preds = clf.predict(raw_test[raw_test.columns[1:]]) # without probability\n",
    "\n",
    "y = raw_test[raw_test.columns[0]]\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y, preds, pos_label=1)\n",
    "metrics.auc(fpr, tpr) # 0.893 in R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8707c1",
   "metadata": {},
   "source": [
    "### Vary cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc4feb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.1         AUC: 0.892400\n",
      "Cost: 1.0         AUC: 0.896311\n",
      "Cost: 10.0        AUC: 0.896222\n"
     ]
    }
   ],
   "source": [
    "for cost in [0.1, 1, 10]:\n",
    "    # fit the svm model\n",
    "    clf = svm.SVC(kernel = 'linear', probability = True, C=cost)\n",
    "    clf.fit(raw_train[raw_train.columns[1:]], raw_train[raw_train.columns[0]])\n",
    "    # make probability predictions\n",
    "    preds = pd.DataFrame(clf.predict_proba(raw_test[raw_test.columns[1:]]))[1]\n",
    "    # get the AUC\n",
    "    y = raw_test[raw_test.columns[0]]\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, preds, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    print(\"Cost: %-10.1f  AUC: %4f\" % (cost, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75c4052",
   "metadata": {},
   "source": [
    "### Vary kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3c4f976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: linear      AUC: 0.896267\n",
      "Kernel: poly        AUC: 0.904578\n",
      "Kernel: rbf         AUC: 0.918933\n",
      "Kernel: sigmoid     AUC: 0.500000\n"
     ]
    }
   ],
   "source": [
    "for kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "    # fit the svm model\n",
    "    clf = svm.SVC(kernel = kernel, probability = True, gamma='auto')\n",
    "    clf.fit(raw_train[raw_train.columns[1:]], raw_train[raw_train.columns[0]])\n",
    "    # make probability predictions\n",
    "    preds = pd.DataFrame(clf.predict_proba(raw_test[raw_test.columns[1:]]))[1]\n",
    "    # get the AUC\n",
    "    y = raw_test[raw_test.columns[0]]\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, preds, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    print(\"Kernel: %-10s  AUC: %4f\" % (kernel, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8d7ca1",
   "metadata": {},
   "source": [
    "We see that the sigmoid kernel function gives back a very low AUC, while in R is gives back a high AUC. We try to investigate why python gives different answers. We check if all cc values are 0 or 1, so that it really is treated as a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c266dd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(raw_train['cc']))\n",
    "print(type(np.unique(raw_train['cc'])[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e38cbc",
   "metadata": {},
   "source": [
    "We try different types of input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cc7629",
   "metadata": {},
   "source": [
    "### Float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f74bce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "raw_train = pd.read_csv(\"simulatedConcentrations.recoded.training.txt\", sep = \"\\t\")\n",
    "raw_test = pd.read_csv(\"simulatedConcentrations.recoded.test.txt\", sep = \"\\t\")\n",
    "\n",
    "raw_train = raw_train.astype(float)\n",
    "raw_test = raw_test.astype(float)\n",
    "print(type(np.unique(raw_train['cc'])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb5aee71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: linear      AUC: 0.896267\n",
      "Kernel: poly        AUC: 0.904622\n",
      "Kernel: rbf         AUC: 0.918933\n",
      "Kernel: sigmoid     AUC: 0.500000\n"
     ]
    }
   ],
   "source": [
    "for kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "    # fit the svm model\n",
    "    clf = svm.SVC(kernel = kernel, probability = True, gamma='auto')\n",
    "    clf.fit(raw_train[raw_train.columns[1:]], raw_train[raw_train.columns[0]])\n",
    "    # make probability predictions\n",
    "    preds = pd.DataFrame(clf.predict_proba(raw_test[raw_test.columns[1:]]))[1]\n",
    "    # get the AUC\n",
    "    y = raw_test[raw_test.columns[0]]\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, preds, pos_label=float(1))\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    print(\"Kernel: %-10s  AUC: %4f\" % (kernel, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd2e6cf",
   "metadata": {},
   "source": [
    "### String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c704dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "raw_train = pd.read_csv(\"simulatedConcentrations.recoded.training.txt\", sep = \"\\t\")\n",
    "raw_test = pd.read_csv(\"simulatedConcentrations.recoded.test.txt\", sep = \"\\t\")\n",
    "\n",
    "raw_train['cc'] = raw_train['cc'].astype(str)\n",
    "raw_test['cc'] = raw_test['cc'].astype(str)\n",
    "print(type(np.unique(raw_train['cc'])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f025a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: linear      AUC: 0.896267\n",
      "Kernel: poly        AUC: 0.904622\n",
      "Kernel: rbf         AUC: 0.918933\n",
      "Kernel: sigmoid     AUC: 0.428089\n"
     ]
    }
   ],
   "source": [
    "for kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "    # fit the svm model\n",
    "    clf = svm.SVC(kernel = kernel, probability = True, gamma='auto')\n",
    "    clf.fit(raw_train[raw_train.columns[1:]], raw_train[raw_train.columns[0]])\n",
    "    # make probability predictions\n",
    "    preds = pd.DataFrame(clf.predict_proba(raw_test[raw_test.columns[1:]]))[1]\n",
    "    # get the AUC\n",
    "    y = raw_test[raw_test.columns[0]]\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, preds, pos_label='1')\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    print(\"Kernel: %-10s  AUC: %4f\" % (kernel, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae313f8",
   "metadata": {},
   "source": [
    "### Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41c01de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "raw_train = pd.read_csv(\"simulatedConcentrations.recoded.training.txt\", sep = \"\\t\", dtype='category')\n",
    "raw_test = pd.read_csv(\"simulatedConcentrations.recoded.test.txt\", sep = \"\\t\", dtype='category')\n",
    "\n",
    "print(type(np.unique(raw_train['cc'])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e63c5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: linear      AUC: 0.896267\n",
      "Kernel: poly        AUC: 0.904622\n",
      "Kernel: rbf         AUC: 0.918889\n",
      "Kernel: sigmoid     AUC: 0.428089\n"
     ]
    }
   ],
   "source": [
    "for kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "    # fit the svm model\n",
    "    clf = svm.SVC(kernel = kernel, probability = True, gamma='auto')\n",
    "    clf.fit(raw_train[raw_train.columns[1:]], raw_train[raw_train.columns[0]])\n",
    "    # make probability predictions\n",
    "    preds = pd.DataFrame(clf.predict_proba(raw_test[raw_test.columns[1:]]))[1]\n",
    "    # get the AUC\n",
    "    y = raw_test[raw_test.columns[0]]\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, preds, pos_label='1')\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    print(\"Kernel: %-10s  AUC: %4f\" % (kernel, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b303695c",
   "metadata": {},
   "source": [
    "### Booleans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3649e259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.bool_'>\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "raw_train = pd.read_csv(\"simulatedConcentrations.recoded.training.txt\", sep = \"\\t\")\n",
    "raw_test = pd.read_csv(\"simulatedConcentrations.recoded.test.txt\", sep = \"\\t\")\n",
    "\n",
    "raw_train['cc'] = raw_train['cc'].astype(bool)\n",
    "raw_test['cc'] = raw_test['cc'].astype(bool)\n",
    "print(type(np.unique(raw_train['cc'])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03a3f3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: linear      AUC: 0.896267\n",
      "Kernel: poly        AUC: 0.904578\n",
      "Kernel: rbf         AUC: 0.918933\n",
      "Kernel: sigmoid     AUC: 0.428089\n"
     ]
    }
   ],
   "source": [
    "for kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "    # fit the svm model\n",
    "    clf = svm.SVC(kernel = kernel, probability = True, gamma='auto')\n",
    "    clf.fit(raw_train[raw_train.columns[1:]], raw_train[raw_train.columns[0]])\n",
    "    # make probability predictions\n",
    "    preds = pd.DataFrame(clf.predict_proba(raw_test[raw_test.columns[1:]]))[1]\n",
    "    # get the AUC\n",
    "    y = raw_test[raw_test.columns[0]]\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, preds, pos_label=True)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    print(\"Kernel: %-10s  AUC: %4f\" % (kernel, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a957a69b",
   "metadata": {},
   "source": [
    "The sigmoid kernel function stays does not increase whatever we try. There might be another cause as to why it gives a different value than R. We try one last thing, another implementation of the sigmoid kernel by sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9295fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "raw_train = pd.read_csv(\"simulatedConcentrations.recoded.training.txt\", sep = \"\\t\")\n",
    "raw_test = pd.read_csv(\"simulatedConcentrations.recoded.test.txt\", sep = \"\\t\")\n",
    "\n",
    "X_train = raw_train[raw_train.columns[1:]]\n",
    "y_train = raw_train[raw_train.columns[0]]\n",
    "X_test = raw_test[raw_test.columns[1:]]\n",
    "y_test = raw_test[raw_test.columns[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93929218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: sigmoid     AUC: 0.624978\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import sigmoid_kernel\n",
    "\n",
    "# fit the svm model\n",
    "clf = svm.SVC(kernel = 'precomputed', probability = True, gamma='auto')\n",
    "kernel_train = sigmoid_kernel(X_train, X_train)\n",
    "\n",
    "clf.fit(kernel_train, y_train)\n",
    "\n",
    "# make probability predictions\n",
    "kernel_test = sigmoid_kernel(X_test, X_train)\n",
    "preds = pd.DataFrame(kernel_test)[1]\n",
    "\n",
    "# get the AUC\n",
    "y = raw_test[raw_test.columns[0]]\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y, preds, pos_label=True)\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "print(\"Kernel: %-10s  AUC: %4f\" % (kernel, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bba6a14",
   "metadata": {},
   "source": [
    "It gives a slightly higher value, but not nearly near ~0.90. We give up!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5497bc6",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f89dae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07af2880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "raw_train = pd.read_csv(\"simulatedConcentrations.recoded.training.txt\", sep = \"\\t\")\n",
    "raw_test = pd.read_csv(\"simulatedConcentrations.recoded.test.txt\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0514937b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7333333333333334"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(raw_train[raw_train.columns[1:]], raw_train[raw_train.columns[0]])\n",
    "\n",
    "y_pred=clf.predict(raw_test[raw_test.columns[1:]])\n",
    "\n",
    "y = raw_test[raw_test.columns[0]]\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y, y_pred, pos_label=1)\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c91143",
   "metadata": {},
   "source": [
    "## Vary n_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6df9903f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trees: 500         AUC: 0.7133\n",
      "Number of trees: 1000        AUC: 0.6967\n",
      "Number of trees: 2500        AUC: 0.6933\n"
     ]
    }
   ],
   "source": [
    "for n_tree in [500, 1000, 2500]:\n",
    "    # fit the svm model\n",
    "    clf = RandomForestClassifier(n_estimators = n_tree)\n",
    "    clf.fit(raw_train[raw_train.columns[1:]], raw_train[raw_train.columns[0]])\n",
    "    y_pred=clf.predict(raw_test[raw_test.columns[1:]])\n",
    "\n",
    "    y = raw_test[raw_test.columns[0]]\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, y_pred, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    print(\"Number of trees: %-10.0f  AUC: %4.4f\" % (n_tree, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8f1378",
   "metadata": {},
   "source": [
    "## Comparison between SVM and RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d1706e",
   "metadata": {},
   "source": [
    "The SVM seems to perform quite a bit better than the RF model. The maximum AUC we managed to get for the SVM model was 0.9188, whereas teh maximum AUC we managed to get for the RF model was 0.7000. The SVM performed 0.918/0.7000*100 = 31.1% better. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b7e888",
   "metadata": {},
   "source": [
    "## Neural network\n",
    "\n",
    "Using code snippets from [PyTorch For Deep Learning — Binary Classification](https://medium.com/analytics-vidhya/pytorch-for-deep-learning-binary-classification-logistic-regression-382abd97fb43), we also train a simple neural net to perform binary classification. We use a logistic regression model and add another layer with multiple neurons to make it a neural net.\n",
    "\n",
    "Note that we use a sigmoid activation function, and we get reasonable results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88932517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "257a8589",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining dataset class\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class dataset(Dataset):\n",
    "      def __init__(self,x,y):\n",
    "        self.x = torch.tensor(x,dtype=torch.float32)\n",
    "        self.y = torch.tensor(y,dtype=torch.float32)\n",
    "        self.length = self.x.shape[0]\n",
    " \n",
    "      def __getitem__(self,idx):\n",
    "        return self.x[idx],self.y[idx]\n",
    "      def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "trainset = dataset(np.array(X_train),np.array(y_train))\n",
    "\n",
    "#dataLoader\n",
    "trainloader = DataLoader(trainset,batch_size=64,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5016520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the network\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,input_shape):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,32)\n",
    "        self.fc2 = nn.Linear(32,64)\n",
    "        self.fc3 = nn.Linear(64,1)\n",
    "    def forward(self,x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x)) # we use sigmoid!\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe59774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters\n",
    "learning_rate = 0.01\n",
    "epochs = 500\n",
    "# Model , Optimizer, Loss\n",
    "model = Net(input_shape=X_train.shape[1])\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2dcd5ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\tloss : 0.6497434377670288\t accuracy : 0.5666666666666667\n",
      "epoch 50\tloss : 0.5734838247299194\t accuracy : 0.5233333333333333\n",
      "epoch 100\tloss : 0.33596426248550415\t accuracy : 0.5033333333333333\n",
      "epoch 150\tloss : 0.14857205748558044\t accuracy : 0.52\n",
      "epoch 200\tloss : 0.07457304000854492\t accuracy : 0.64\n",
      "epoch 250\tloss : 0.04842297360301018\t accuracy : 0.7566666666666667\n",
      "epoch 300\tloss : 0.036096833646297455\t accuracy : 0.8233333333333334\n",
      "epoch 350\tloss : 0.027203576639294624\t accuracy : 0.8166666666666667\n",
      "epoch 400\tloss : 0.020289259031414986\t accuracy : 0.8066666666666666\n",
      "epoch 450\tloss : 0.006906598806381226\t accuracy : 0.7766666666666666\n"
     ]
    }
   ],
   "source": [
    "#forward loop\n",
    "losses = []\n",
    "accur = []\n",
    "for i in range(epochs):\n",
    "      for j,(x_train,y_train) in enumerate(trainloader):\n",
    "    \n",
    "        #calculate output\n",
    "        output = model(x_train)\n",
    "\n",
    "        #calculate loss\n",
    "        loss = loss_fn(output,y_train.reshape(-1,1))\n",
    "\n",
    "        #accuracy\n",
    "        predicted = model(torch.tensor(np.array(X_test),dtype=torch.float32))\n",
    "        acc = (predicted.reshape(-1).detach().numpy().round() == y).mean()\n",
    "        #backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "      if i%50 == 0:\n",
    "        losses.append(loss)\n",
    "        accur.append(acc)\n",
    "        print(\"epoch {}\\tloss : {}\\t accuracy : {}\".format(i,loss,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59567edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting the loss\n",
    "losses = [loss.detach().numpy() for loss in losses]\n",
    "plt.plot(np.linspace(0, 500, num=10), losses)\n",
    "plt.title('Loss vs Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "822f6cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#printing the accuracy\n",
    "plt.plot(accur)\n",
    "plt.title('Accuracy vs Epochs')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
