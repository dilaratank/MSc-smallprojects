---
title: "MAM11 Prediction Models Notebook"
output: html_notebook
---

In this assignment we will develop and evaluate prediction models for breast cancer. The dataset is provided in the package "mlbench" so you do not need to download it from an explicit address.

What should you submit? You need to submit:
1. This Notebook (in .Rmd format) after filling in all your answers inside this notebook. You are required to do an action (write or fill-in code or write documentation) at the places marked by an arrow (=>). This arrow appears either in the comments (and it will be preceeded by "#") or outside the text (without "#"). One of the actions is to document a chunk of code. Inside the code you need to document the commands marked with "#@".
2. The Notebook in html format with all the results.

Grading: The practical has two parts. The first is a "closed" part, in which you are expected to add some code or to document a chunk of code. The other part is open, in which you choose a non-parametric approach to learn models, and you compare a strategy based on this new approach to a strategy that you learn during the closed part. The closed part amounts to 70% of the grade and the open part amounts to 30% of the grade. Of course we are more than happy to help you (also) on the open part but it is emphatically intended to be your own work.

Good luck!

==> Student Name(s): Martijn Siepel and Dilara Tank
==> Student number(s): 12892858, 12170062

Load libraries
```{r, echo=FALSE}
library(tidyverse) # Includes many libraries like dplyr (for easy data maniputaion, readr (for reading datasets), ggplot2 (for creating elegant data visualisations), tibble (for easily working with data frames), etc.)
library(rms)       # for implementing Regression Modeling Strategies
library(ROCR)      # for visualizing the performance of prediction models
library(pROC)      # for Analyzing ROC curves
library(mlbench)   # for provision of machine learning benchmark problems
library(MASS)      # for stepwise variable selection
library(stringr)
library(rpart)
library(Metrics)
library(mlr)
```

Set options
```{r, echo=FALSE}
options(dplyr.print_min = 5L, dplyr.print_max = 5L) # set number of rows to show in a data frame
```

# read file
```{r, message=FALSE}
data(BreastCancer, package="mlbench")
names(BreastCancer) # * What are the names of the data frame bc? Id, Cl.thickness, Cell.size, Cell.shape, Marg.adhesion, Epith.c.size, Bare.nuclei, Bl.cromatin, Normal.nucleoli, Mitoses, Class          

nrow(BreastCancer)  # * How many obseravtion (rows) are there? There are 700 rows.
# Take a moment to look at the description of the database on http://ugrad.stat.ubc.ca/R/library/mlbench/html/BreastCancer.html

# Print the dataset
BreastCancer

# => Are there any missing values? There are 16 missing values
sum(is.na(BreastCancer))

# => Write the code to calculate the number of missing values in each column. You can use whatever suits you. For example you can use "sapply" on BreastCancer to work on all columns while using the is.na function, or (preferably) you can use the power of dplyr commands like "summarise" together with sum and "is.na".
colSums(is.na(BreastCancer))

```

Preprocess
```{r}
# => We will use only complete cases without any missing values in this excercise. Obtain the data frame "bc" with complete cases. Hint: look at the function complete.cases(). You can use it for example with the filter command of dplyr (or just use base R)
bc <- na.omit(BreastCancer)
# => How many obseravtion (rows) are there now? 683 rows 
nrow(bc)
# How many cases did we remove? We removed the 16 rows because Bare.nuclei had missing values.
nrow(BreastCancer) - nrow(bc)

# remove id column
bc <- bc[,-1] #=> Remove the "id" column from bc. Note that if you consider to use the "select" command in dplyr then this command may clash with the select command in the MASS library. Therefore use dplyr::select in that case.

# => convert the first 9 factors to numeric. You can use a for loop on these variables, or use "mutate_at". 
bc <- bc %>% mutate_at(1:9, as.numeric)

# Look at the class variable
bc$Class

bc$Class <- ifelse(bc$Class == 'malignant', 1, 0)
bc$Class
# => change "malignant" into the number 1, and "bening" into the number 0. You can use a simple ifelse, or use the "recode" command in dplkr.
```


We will work with logistic regression models in this assignment to predict the probability of the malignant class (class = 1), but we could have used other kinds of models. We will fit two logistic regression models: one with only Mitoses as covariate, and another model with Mitoses and Cl.thickness. We will use the "lrm" command in the "rms" package but we could have also used the base R command "glm" instead. We will see later how to use glm.
```{r}
ddist <- datadist(bc) # preparation: in package rms we need to define the data distribution of the variables first. So before any models are fitted, this command stores the distribution summaries (e.g. ranges) for all potential variables.  
options(datadist='ddist') # This means that when we fit a model we will also store the distribution information with the model object as well
mitoses.lrm <- lrm(Class~Mitoses, x=T, y=T, data=bc) # x = T and y=Y mean that the object mitoses.lrm will not only inlcude the model but will also keep the data as well. This is useful when we want access to the data via the model object.

summary(mitoses.lrm)
mitoses.thickness.lrm <- lrm(Class ~ Mitoses + Cl.thickness, x=T, y=T, data=bc) # fit a model that includes Mitoses and Cl.thickness as covariates
```


What are the model's characteristics on a test set? Let's create a training and test sets and train the two models on the training set

```{r}
set.seed(1234) # we fix the seed so that results are the same for all students
smp_size <- floor(0.70 * nrow(bc))
train_ind <- sample(seq_len(nrow(bc)), size = smp_size)

train <- bc[train_ind, ] # => Obtain train set consisting of 70% of the data
test <- bc[-train_ind, ] #=> Obtain test set consisting of the rest of observations

mitoses.train.lrm <- lrm(Class ~ Mitoses, x=T, y=T, data=train) # => fit lrm model on the training set using only Mitoses, use x=T and y=T
mitoses.thickness.train.lrm <-lrm(Class ~ Mitoses + Cl.thickness, x=T, y=T, data=train) # => fit lrm model on the training set using  Mitoses and Cl.thickness, use x=T and y=T
```

Now predict on the test set
```{r}
predicted.mitoses.test <- predict(mitoses.train.lrm, test, type='fitted') # => obtain the predicted probabilities of mitoses.train.lrm on the test set. Hint: use the "predict" function. Important note: make sure you use the right "type" in the command to get probabilities and not log odds.
predicted.mitoses.thickness.test <- predict(mitoses.thickness.train.lrm, test, type='fitted') # => obtain the predicted probabilities of mitoses.thickness.train.lrm on the test set.Check that they indeed are between 0 and 1 and have no negative numbers etc.
mean(predicted.mitoses.test)
min(predicted.mitoses.test)
max(predicted.mitoses.test)
```
Inspect histogram and ranges of probabilities
```{r}
 # => plot histogram of predicted.mitoses.test
hist(predicted.mitoses.test)
 #* => plot histogram of predicted.mitoses.thickness.test
hist(predicted.mitoses.thickness.test)
 #* => Obtain range of predicted.mitoses.test
range(predicted.mitoses.test)
max(predicted.mitoses.test) - min(predicted.mitoses.test)
 #* => Obtain range of predicted.mitoses.thickness.test
range(predicted.mitoses.thickness.test)
max(predicted.mitoses.thickness.test) - min(predicted.mitoses.thickness.test)
```

Q: => From the point of view of larger range, which model is better?
The second model. This model has a higher range meaning the probabilities for outcome 1/0 are farther spread, meaning there's a higher discrimination.


Probability densities
```{r}
mitoses.test.pred.class <- data.frame(pr=predicted.mitoses.test, cl = as.factor(test$Class))
ggplot(mitoses.test.pred.class, aes(pr, fill = cl)) + geom_density(adjust = 2, alpha = 0.5) + xlab("predicted probability")
# => For this density plot, which of the following statements are true:
# For those without breast cancer the probabilities are concentrated below 0.4 -> True
# For those with breast cancer the probabilities are concentrated above 0.8 -> False
# For those without breast cancer the probabilities are very high -> False
# For those with breast cancer they are likely to have any probability. -> True

# => What do you think that the "adjust" above does. Try the value 1 instead of 2. What does alpha do? Try alpha = 1
ggplot(mitoses.test.pred.class, aes(pr, fill = cl)) + geom_density(adjust = 2, alpha = 0.5) + xlab("predicted probability")
# The adjust parameter influences the amount of smoothing 
# The alpha parameter influences the transparency of the surface below the lines

# => plot the probability density graph as before but now for the predicted.mitoses.thickness.test
mitoses.thickness.test.pred.class <- data.frame(pr=predicted.mitoses.thickness.test, cl = as.factor(test$Class))
ggplot(mitoses.thickness.test.pred.class, aes(pr, fill = cl)) + geom_density(adjust = 2, alpha = 0.5) + xlab("predicted probability")

# => Calculate the discrimination slope for both models. Ypu may want to consult the slides of the presentation to recall what that is. Which model is better in its discrimination slope?
# Discrimintation is the difference between the means of event = 0/1

# Means mitoses.test.pred.class 
mean(mitoses.test.pred.class$pr[mitoses.test.pred.class$cl == 1]) - mean(mitoses.test.pred.class$pr[mitoses.test.pred.class$cl == 0])

# Means mitoses.thickness.test.pred.class
mean(mitoses.thickness.test.pred.class$pr[mitoses.thickness.test.pred.class$cl == 1]) - mean(mitoses.thickness.test.pred.class$pr[mitoses.thickness.test.pred.class$cl == 0])


# Calculate the unsharpness of both models. Look at the slides in the presentation. Which model is sharpner (i.e. less unsharpness)? 

# sum of all predictions
unsharpness.mitoses.test <- sum(predicted.mitoses.test * (1 - predicted.mitoses.test)) / length(predicted.mitoses.test)
unsharpness.mitoses.test

# sum of all predictions
unsharpness.mitoses.thickness.test <- sum(predicted.mitoses.thickness.test * (1 - predicted.mitoses.thickness.test)) / length(predicted.mitoses.thickness.test) 
unsharpness.mitoses.thickness.test

# the second model is sharper, the unsharpness is closer to 0. 
```

Let's look at the ROC curve and the Area Under the ROC Curve (AUC).
The functions "prediction" and "performance" are from the ROCR package.
```{r}
pred.mitoses.test <- prediction(predicted.mitoses.test, test$Class) # Specify the predictions and observed outcome
perf.mitoses.test <- ROCR::performance(pred.mitoses.test,"tpr","fpr") # Specify what to calculate, in our case tpr and fpr.
plot(perf.mitoses.test, colorize=F, col="green")
abline(0, 1)

pred.mitoses.thickness.test <- prediction(predicted.mitoses.thickness.test, test$Class)
perf.mitoses.thickness.test <- ROCR::performance(pred.mitoses.thickness.test,"tpr","fpr")
plot(perf.mitoses.thickness.test, add=T, colorize=F, col="red") # Note the "add=T"  in order to plot on a pre existing plot

# Calculate the AUC for both models according to the "social party" we discussed in class (the proportion of times from all pairs in which the person with the event got higher probability of the event than the person without the event). Verify that you get the same results

calculateAUC <- function (pred.class) {
  pred.class.ordered <- pred.class[order(pred.class$pr),] # order the data
  outer_count = 0
  for (row in 1:nrow(pred.class.ordered)) {
    cl <- pred.class.ordered[row, "cl"]
    if(cl == 1) { # if there's a class = class, loop through all earlier rows and count the number of occurrences of 0
      inner_count_benign = 0
      for (inner_row in 1:nrow(pred.class.ordered[1:row-1,])) {
        class <- pred.class.ordered[inner_row, "cl"]
        if (class == 0) {
          inner_count_benign = inner_count_benign + 1
        }
      }
      outer_count = outer_count + inner_count_benign # sum all these occurrences
    }
  }
  total_pars <- data.frame(table(pred.class$cl))$Freq[1]*data.frame(table(pred.class$cl))$Freq[2]  # Total pairs 
  pred.class.auc <- outer_count/total_pars # calculate the auc 
  return (pred.class.auc)
}

calculateAUC(mitoses.test.pred.class)
calculateAUC(mitoses.thickness.test.pred.class)

```
=> Which model is better from the AUC point of view?
The model with mitoses and thickness has a higher AUC, meaning the higher probabilites are more often with the class = malignant

Let's look at the NRI (net Reclassification Improvement)
```{r}
# => Calculate the NRI for those with malignant breast cancer when using mitoses.thickness.train.lrm compared to mitoses.train.lrm. You need to know how many times their probability improved (got higher) with the  mitoses.thickness.train.lrm model, and how many times it worsened. The difference between these two is the net improvement. You can then divide this difference by the number of patients with malignant breast cancer to obtain the proportion. This proportion is the NRI for those in class = 1.

# => Calculate the NRI for those with NO malignant breast cancer when using mitoses.thickness.train.lrm compared to mitoses.train.lrm.
df1 <- data.frame(mitoses.thickness.test.pred.class[mitoses.thickness.test.pred.class$cl == 1, ]$pr)
df2 <- data.frame(mitoses.test.pred.class[mitoses.test.pred.class$cl == 1, ]$pr)

nri_table <- ifelse(Reduce(`|`, Map(`>`, df1, df2)), 'Improved', 'Not Improved')
not_improved <- sum(str_count(nri_table, "Not Improved"))
improved <- length(nri_table) - not_improved

nri_class_1 <- (improved - not_improved) / length((nri_table))
nri_class_1

# => Calculate the NRI for those with NO malignant breast cancer when using mitoses.thickness.train.lrm compared to mitoses.train.lrm.

df1 <- data.frame(mitoses.thickness.test.pred.class[mitoses.thickness.test.pred.class$cl == 0, ]$pr)
df2 <- data.frame(mitoses.test.pred.class[mitoses.test.pred.class$cl == 0, ]$pr)

nri_table <- ifelse(Reduce(`|`, Map(`<`, df1, df2)), 'Improved', 'Not Improved')
not_improved <- sum(str_count(nri_table, "Not Improved"))
improved <- length(nri_table) - not_improved

nri_class_0 <- (improved - not_improved) / length((nri_table))
nri_class_0
```

Now let us look at calibration graphs. We will use "loess" to smooth the data. The parameter "span" controls the amount of smoothing

```{r}
predicted <- predicted.mitoses.test # predicted.mitoses.test includes the probabilities according to the model that uses only mitoses
loess.model <- loess(test$Class ~ predicted, span = 1) # make loess model to smooth the Class information (which is 0 and 1). This gives, for each prediction, a proportion of subjects with breast cancer
proportionCancer <- predict(loess.model) # obtain the smoothed predictions by loess
ind <- order(predicted) # index of lowest to highest prediction
xy.predicted.mitoses.test <- data.frame(x=predicted[ind], y= proportionCancer[ind])
ggplot(xy.predicted.mitoses.test, aes(x=x, y=y)) + geom_line() + geom_abline(intercept=0, slope=1, col="red") + xlab("Predicted probabilities") + ylab("Probability of observed breast cancer")

# => Plot calibration graph for the probabilities predicted.mitoses.thickness.test
predicted <- predicted.mitoses.thickness.test # Now we use the other model

loess.model <- loess(test$Class ~ predicted, span = 1) # make loess model to smooth the Class information (which is 0 and 1). This gives, for each prediction, a proportion of subjects with breast cancer
proportionCancer <- predict(loess.model) # obtain the smoothed predictions by loess
ind <- order(predicted) # index of lowest to highest prediction
xy.predicted.mitoses.test <- data.frame(x=predicted[ind], y= proportionCancer[ind])
ggplot(xy.predicted.mitoses.test, aes(x=x, y=y)) + geom_line() + geom_abline(intercept=0, slope=1, col="red") + xlab("Predicted probabilities") + ylab("Probability of observed breast cancer")

```
=> Which model do you prefer in terms of calibration
The first model has a lower difference between the predictions and the actual probabilities. However, the second model predicts better as a whole because the first model performs relatively poorly on patients with a high actual probability of malignant cancer.

=> Suppose we want to implement your models in clinical practice. Someone suggests that when the predicted probabilityis equal or exceeds 30% then the subject is treated as if the patient has breast cancer and referred to additional work up (diagnostics and therapy). Considering the test set and each model separately, how many patients would be incorrectly referred to further work up and how many incorrectly labeled as not having breast cancer. Would you suggest to increase or decrease the cut-off point? Motivate your answer.

For the AUC and Brier score let's be more rigorous and calculate them using bootstrapping in order to use all data (without needing to split) and to also get CIs (confidence intervals) around their estimates. You do not need to change the code, it is there for you to learn from. => What is required from you is to DOCUMENT the code. This is the best way to learn it. You are required to write a comment just above any command marked with "#@" 

Specify formula for the model you want to work with
```{r}
my.formula <- formula(Class ~ Cell.shape + Cell.size) # We could have used the same variables as before but let's try other variables.
```

Record AUC and Brier score of the model on the (original training) dataset. This will be needed in the next step.
```{r}
my.glm <- glm(my.formula, family="binomial", data=bc) # This fits a logistic regression model. We demonstrate the use of glm (instead of lrm) which is part of base R (no need for a special package). Note that we specify "binomial" if we want a logistic regression model with glm 
predictions.training <- predict(my.glm, type="response") # and we use "type = response" to get the predicted probabilities. This is different than in lrm. Just be aware of the differences.
hist(predictions.training) 
pred.train <- prediction(predictions.training, bc$Class) # Again we use "prediction"" in ROCR
auc.train <- ROCR::performance(pred.train, "auc") # we ask for the auc this time. This provides an object that includes the AUC
AUC.ORIGINAL <- slot(auc.train, "y.values")[[1]] # We need to obtain the AUC from this object
BRIER.ORIGINAL <- mean((predictions.training - (bc$Class))^2) # This is a quick implementation of the Brier Score. Try to understand why this is consistent with what we learned in the class.
```

APPLY BOOTSTRAP: Now we will apply bootstrapping to calculate the corrected AUC and Brier, and importantly their CI
```{r, echo=FALSE, message=FALSE}
# Let us initialize vector variables. We will fill the vectors during the for-loop below.
AUCS.m <<- vector(mode="integer", length=0) # Vector to save the AUCs of bootsrtap models tested in their own boostrap sample.
# Note that we use "<<-" rather than the normal assignment "<-". The difference is that "<<-" will save the results
# in a global environment so even when the for-loop is finished the values assigned to the variables in the loop
# will be known outside the loop. Otherwise if we use "<-" then these values are not rememberd outside the loop.  
BRIERS.m <<- vector(mode="integer", length=0)  # Vector for the Brier scores on the bootstrap samples
AUCS.orig <<- vector(mode="integer", length=0)  # Vector for the AUCs of bootsrtrap models tested on the original dataset
BRIERS.orig <<- vector(mode="integer", length=0)  # Vector for the Briers of bootsrtrap models tested on the original dataset
for (i in 1:500){
  index <- sample(1:nrow(bc), replace=T) #@ Gather the indexes of the bootstrap resampling performed on a vector that is the same length as the bc dataset
  bootsmpl <- bc[index,] #@ Get the actual values from the bc dataset with the indexes, this is the bootstrap sample
  boot.m <- glm(my.formula, family="binomial", data=bootsmpl) #@ Fit a logistic regression model with glm using binomial 
  probs.m <- predict(boot.m, type="response") #@ Get the probabilities/predictions on the bootstrapped dataset
  probs.orig <- predict(boot.m, newdata = bc, type="response") #@ Get the probabilities/predictions on the original dataset

  pred.m <- ROCR::prediction(probs.m, bootsmpl$Class)
  auc.m <- ROCR::performance(pred.m, "auc") #@ Calculate the AUC of the predictions on the bootstrapped dataset in the form of a performance object
  a.m <- slot(auc.m, "y.values")[[1]] #@ Read the performance object (the actual value of the AUC)
  AUCS.m <<- c(AUCS.m, a.m) #@ Combine the AUC values into a vector
  brier.m <- mean((probs.m - bootsmpl$Class)^2) #@ Calculate the Brier score of the predictions on the bootstrapped dataset
  BRIERS.m <<- c(BRIERS.m, brier.m) #@ Combine the Brier scores into a vector
  pred.orig <- ROCR::prediction(probs.orig, bc$Class)
  auc.orig <- ROCR::performance(pred.orig, "auc") #@ Get the AUC of the predictions on the original dataset
  a.orig <- slot(auc.orig, "y.values")[[1]] #@ Read the instance
  brier.orig <- mean((probs.orig - (bc$Class))^2) #@ Calculate the Brier score of the predictions on the original dataset
  AUCS.orig <<- c(AUCS.orig, a.orig) #@ Combine the AUC values into a vector
  BRIERS.orig <<- c(BRIERS.orig, brier.orig) #@ Combine the Brier scores into a vector
}

mean(AUCS.m)
CIauc <- quantile(AUCS.m-AUCS.orig, probs=c(0.025, 0.975)) #@ Calculates the confidence interval of the collected AUCs to see if there is a significant difference between the two models
CIbrier <- quantile(BRIERS.m - BRIERS.orig, probs=c(0.025, 0.975)) #@ Calculates the confidence interval of the collected Brier scores to see if there is a significant difference between the two models


AUC.ORIGINAL - mean(AUCS.m - AUCS.orig) #@ Compute error on bootstrap sample and on original sample and subtract it from the original AUC to calcualte the AUC corrected for optimism 
quantile(AUC.ORIGINAL - (AUCS.m - AUCS.orig), probs=c(0.025, 0.975)) #@ Compute error on bootstrap sample and on original sample in terms of confidence intervals to see if there is a significant difference between the two models

BRIER.ORIGINAL - mean(BRIERS.m - BRIERS.orig) #@ Compute error on bootstrap sample and on original sample and subtract it from the original Brier to calcualte the AUC corrected for optimism 
quantile(BRIER.ORIGINAL - (BRIERS.m - BRIERS.orig), probs=c(0.025, 0.975)) #@ Compute error on bootstrap sample and on original sample in terms of confidence intervals to see if there is a significant difference between the two models

# the corrected AUC is 0.9823195
```

Up til now we have selected the predictor variables ourselves, but which are the best Class predictors? We need to control for complexity of the model. One way to do it is to use the Akaike Information Criterion.
```{r}
all.glm <- glm(Class~ . -Class, family=binomial, data=bc) # use all predictors (exclude Class from predictors)
stepAIC(all.glm, direction="backward")
#=> which variables are selected? Cl.thickness + Cell.shape + Marg.adhesion + Bare.nuclei + Bl.cromatin + Normal.nucleoli + Mitoses
#=> Try direction "forward" and then "both". Do you get the same results?
stepAIC(all.glm, direction="forward") # chosen variables: Cl.thickness + Cell.size + Cell.shape + Marg.adhesion + Epith.c.size + Bare.nuclei + Bl.cromatin + Normal.nucleoli + Mitoses, different results
stepAIC(all.glm, direction="both") # chosen variables: Cl.thickness + Cell.shape + Marg.adhesion + Bare.nuclei + Bl.cromatin + Normal.nucleoli + Mitoses, different results
```

Important note: if we wanted to test the strategy for obtaining the best possible model (that is, the model with the best variables) then we need to include the stepAIC variable AIC selection procedure above inside the bootstrap procedure. In other words we would need to add the StepAIC inside the block "APPLY BOOTSTRAP" above.

=> implement the variable selection strategy with stepAIC (use the direction "bakward") inside the bootstrap sample. What is the optimism-corrected AUC of this strategy? What is the final model?

The optimism-corrected AUC of this strategy is 0.9813531
The final model in this code is boot.m.step. In reality, you would want to train this model on all the data
and present that as your final dataset, as you have proven that the optimism corrected AUC is more ore less
the same as the original AUC. 

```{r, message=FALSE, warning = FALSE}
AUCS.m <<- vector(mode="integer", length=0)  
AUCS.orig <<- vector(mode="integer", length=0)
for (i in 1:500){
  index <- sample(1:nrow(bc), replace=T) 
  bootsmpl <- bc[index,] 
  
  boot.m <- glm(Class~ . -Class, family=binomial, data=bootsmpl) 
  invisible(boot.m.step <- stepAIC(boot.m, direction="backward", trace=FALSE))
  
  probs.m <- predict(boot.m.step, type="response") 
  probs.orig <- predict(boot.m.step, newdata = bc, type="response") 

  pred.m <- ROCR::prediction(probs.m, bootsmpl$Class)
  auc.m <- ROCR::performance(pred.m, "auc") 
  a.m <- slot(auc.m, "y.values")[[1]] 
  AUCS.m <<- c(AUCS.m, a.m) 
  auc.orig <- ROCR::performance(pred.orig, "auc")
  a.orig <- slot(auc.orig, "y.values")[[1]] 
  AUCS.orig <<- c(AUCS.orig, a.orig)
}

mean(AUCS.m)
CIauc <- quantile(AUCS.m-AUCS.orig, probs=c(0.025, 0.975)) 
CIauc

AUC.ORIGINAL - mean(AUCS.m - AUCS.orig) 
quantile(AUC.ORIGINAL - (AUCS.m - AUCS.orig), probs=c(0.025, 0.975)) 

```


=> OPEN question (30% of the grade): implement a strategy based on a non parametric model of your choice like a decision tree or random forests. Compare, in terms of the AUCs, of the strategy based on logistic regression with stepAIC to the new strategy (decision tree of random forests or whatever you like). Show clearly if and how you control for complexity in the new strategy and how do you tell whether the differences in performance are statistically significant? You need to find out answers to these questions yourself.

The AUC of the strategy based on logistic regression with stepAIC produced an AUC of 0.9867402, our approach of Decision trees with handling complexty produced an AUC of: 0.9585848. The logistic regression strategy had a higher AUC, which can be explained due to the fact that logistic regression models often work quite good when applied to predict he categorical dependent variable (when the prediction is categorical, which in our case it was!).

We produce this code using snippets from https://towardsdatascience.com/decision-tree-hyperparameter-tuning-in-r-using-mlr-3248bfd2d88c. 

```{r}

set.seed(1234) # we fix the seed 
smp_size <- floor(0.70 * nrow(bc))
train_ind <- sample(seq_len(nrow(bc)), size = smp_size)

train <- bc[train_ind, ] # => Obtain train set consisting of 70% of the data
test <- bc[-train_ind, ] #=> Obtain test set consisting of the rest of observations

# We define the parameters we want to optimize for, to decrease model complexity
train$Class <- as.factor(train$Class)
mytree.params <- makeClassifTask(data=train, target='Class') 

control_grid = makeTuneControlGrid() 
resample = makeResampleDesc("CV", iters = 5L) # 5 fold cross validation
measure = acc

param_grid_multi <- makeParamSet(
 makeDiscreteParam("maxdepth", values=1:5),
 makeNumericParam("cp", lower = 0.001, upper = 0.005),
 makeDiscreteParam("minsplit", values=1:5)
)
# we used the following parameters:
 # param_grid_multi <- makeParamSet(
 #  makeDiscreteParam("maxdepth", values=1:30),
 #  makeNumericParam("cp", lower = 0.001, upper = 0.01),
 #  makeDiscreteParam("minsplit", values=1:30)
 # )

# However, this took quite long to run. So in this notebook we lowered these values to
# save your time :))

 # We tune the parameters
dt_tuneparam_multi <- tuneParams(learner='classif.rpart', 
 task=mytree.params, 
 resampling = resample,
 measures = measure,
 par.set=param_grid_multi, 
 control=control_grid)#,show.info = TRUE)
 
# We find the best parameters
best_parameters_multi = setHyperPars(
 makeLearner("classif.rpart", predict.type = "prob"), 
 par.vals = dt_tuneparam_multi$x
)

best_parameters_multi
```

```{r}
# We use the best parameters to fit a decision tree
mytree_control <-  rpart(formula = Class ~ . - Class, data = train, method = "class", parms = list(split ="information"), control =rpart.control(maxdepth=4,cp=0.006,minsplit=3))

# Get the predictions and calculate the AUC
predicted_values_control <- predict(mytree_control, test, type = "class")
auc(test$Class, predicted_values_control)

# AUCS.m: 0.9867402, Decision tree: 0.9585848
```